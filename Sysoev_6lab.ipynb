{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a87b1e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df_sample = df.sample(n=3000, random_state=42)\n",
    "df_sample.to_csv('IMDB Dataset_edit3000.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70077755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/12/dd/f17b11a93a9ca27728e12512d167eb1281c151c4c6881d3ab59eb58f4127/transformers-4.35.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
      "     -------------------------------------- 123.5/123.5 kB 1.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/05/09/1945ca6ba3ad8ad6e2872ba682ce8d68c5e63c8e55458ed8ab4885709f1d/huggingface_hub-0.19.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.19,>=0.14 from https://files.pythonhosted.org/packages/bb/0f/8d583f3aac636dca3287a8ba5ef12cd898526ee5f8cffa7348a7a0a7ac76/tokenizers-0.15.0-cp38-none-win_amd64.whl.metadata\n",
      "  Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4f/02/df46e05fa614751e13e1dbfa8fd8832944723d757ac16fc9074f4df940be/safetensors-0.4.1-cp38-none-win_amd64.whl.metadata\n",
      "  Downloading safetensors-0.4.1-cp38-none-win_amd64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec>=2023.5.0 from https://files.pythonhosted.org/packages/96/0e/2be9b5a2e3f736577e749bbdf27a1e7e965041e1c908d49dedf56eeb2b8a/fsspec-2023.12.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.12.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
      "   ---------------------------------------- 7.9/7.9 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 311.7/311.7 kB 1.5 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.1-cp38-none-win_amd64.whl (277 kB)\n",
      "   ---------------------------------------- 277.7/277.7 kB 1.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.0-cp38-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2023.12.1-py3-none-any.whl (168 kB)\n",
      "   ---------------------------------------- 168.9/168.9 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, fsspec, huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed fsspec-2023.12.1 huggingface-hub-0.19.4 safetensors-0.4.1 tokenizers-0.15.0 transformers-4.35.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4bff508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\invet\\anaconda3\\lib\\site-packages (23.2.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for torch: [Errno 2] No such file or directory: 'c:\\\\users\\\\invet\\\\anaconda3\\\\lib\\\\site-packages\\\\torch-2.0.1.dist-info\\\\METADATA'\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting pip\n",
      "  Obtaining dependency information for pip from https://files.pythonhosted.org/packages/47/6a/453160888fab7c6a432a6e25f8afe6256d0d9f2cbd25971021da6491d899/pip-23.3.1-py3-none-any.whl.metadata\n",
      "  Downloading pip-23.3.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.2.1\n",
      "    Uninstalling pip-23.2.1:\n",
      "      Successfully uninstalled pip-23.2.1\n",
      "Successfully installed pip-23.3.1\n"
     ]
    }
   ],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52e19e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext\n",
      "  Downloading torchtext-0.16.1-cp38-cp38-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torchtext) (4.59.0)\n",
      "Requirement already satisfied: requests in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torchtext) (2.25.1)\n",
      "Collecting torch==2.1.1 (from torchtext)\n",
      "  Downloading torch-2.1.1-cp38-cp38-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torchtext) (1.22.4)\n",
      "Collecting torchdata==0.7.1 (from torchtext)\n",
      "  Downloading torchdata-0.7.1-cp38-cp38-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (4.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (1.8)\n",
      "Requirement already satisfied: networkx in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2.11.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torch==2.1.1->torchtext) (2023.12.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from torchdata==0.7.1->torchtext) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->torchtext) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from requests->torchtext) (2020.12.5)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from jinja2->torch==2.1.1->torchtext) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from networkx->torch==2.1.1->torchtext) (5.0.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\invet\\anaconda3\\lib\\site-packages (from sympy->torch==2.1.1->torchtext) (1.2.1)\n",
      "Downloading torchtext-0.16.1-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading torch-2.1.1-cp38-cp38-win_amd64.whl (192.3 MB)\n",
      "   ---------------------------------------- 192.3/192.3 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading torchdata-0.7.1-cp38-cp38-win_amd64.whl (1.3 MB)\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchdata, torchtext\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for torch: [Errno 2] No such file or directory: 'c:\\\\users\\\\invet\\\\anaconda3\\\\lib\\\\site-packages\\\\torch-2.0.1.dist-info\\\\METADATA'\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "    WARNING: No metadata found in c:\\users\\invet\\anaconda3\\lib\\site-packages\n",
      "ERROR: Cannot uninstall torch 2.0.1, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps torch==2.0.1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85d5a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c929eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDB Dataset_edit3000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e199524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество положительных отзывов: 1491\n"
     ]
    }
   ],
   "source": [
    "positive_reviews_count = df[df['sentiment'] == 'negative'].shape[0]\n",
    "\n",
    "print(f\"Количество положительных отзывов: {positive_reviews_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "938b4b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I really liked this Summerslam due to the look...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not many television shows appeal to quite as m...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The film quickly gets to a major chase scene w...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jane Austen would definitely approve of this o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Expectations were somewhat high for me when I ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I've watched this movie on a fairly regular ba...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>For once a story of hope highlighted over the ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Okay, I didn't get the Purgatory thing the fir...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I was very disappointed with this series. It h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The first 30 minutes of Tinseltown had my fing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jeez, this was immensely boring. the leading m...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Great just great! The West Coast got \"Dirty\" H...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>It's made in 2007 and the CG is bad for a movi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>This movie stinks majorly. The only reason I g...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>We can start with the wooden acting but this f...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>This movie starts off somewhat slowly and gets...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>This is a slightly uneven entry with one stand...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I was first introduced to John Waters films by...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>This movie has very good acting by virtually a...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I can't help but notice the negative reviews t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review sentiment\n",
       "0   I really liked this Summerslam due to the look...  positive\n",
       "1   Not many television shows appeal to quite as m...  positive\n",
       "2   The film quickly gets to a major chase scene w...  negative\n",
       "3   Jane Austen would definitely approve of this o...  positive\n",
       "4   Expectations were somewhat high for me when I ...  negative\n",
       "5   I've watched this movie on a fairly regular ba...  positive\n",
       "6   For once a story of hope highlighted over the ...  positive\n",
       "7   Okay, I didn't get the Purgatory thing the fir...  positive\n",
       "8   I was very disappointed with this series. It h...  negative\n",
       "9   The first 30 minutes of Tinseltown had my fing...  negative\n",
       "10  jeez, this was immensely boring. the leading m...  negative\n",
       "11  Great just great! The West Coast got \"Dirty\" H...  positive\n",
       "12  It's made in 2007 and the CG is bad for a movi...  negative\n",
       "13  This movie stinks majorly. The only reason I g...  negative\n",
       "14  We can start with the wooden acting but this f...  negative\n",
       "15  This movie starts off somewhat slowly and gets...  positive\n",
       "16  This is a slightly uneven entry with one stand...  positive\n",
       "17  I was first introduced to John Waters films by...  positive\n",
       "18  This movie has very good acting by virtually a...  positive\n",
       "19  I can't help but notice the negative reviews t...  positive"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed4ea8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b7a2e75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f661e77e76ed447facfc8ae83866f723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b20ad940615469c85b7eb4b560ea3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a63fc1b48e4b97807d2133bca32b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e7a894f43e4154ae3ab5e7ed3dc2c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16c2e6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "60c9b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████████████████████| 2400/2400 [00:09<00:00, 258.18it/s]\n",
      "Tokenizing: 100%|████████████████████████████| 600/600 [00:02<00:00, 266.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_data(texts, labels, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "        encoded_text = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            truncation=True,  # Добавьте этот параметр\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids.append(encoded_text['input_ids'])\n",
    "        attention_masks.append(encoded_text['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels.values, dtype=torch.long)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "\n",
    "# Токенизация тренировочных и валидационных данных\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(X_train, y_train)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(X_val, y_val)\n",
    "\n",
    "# Создание загрузчиков данных\n",
    "batch_size = 16\n",
    "train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "25b23dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Validation Accuracy: 0.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Validation Accuracy: 0.8233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Validation Accuracy: 0.8100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Определение параметров обучения\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Обучение модели\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        val_preds.extend(predictions)\n",
    "        val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50140603",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = DistilBertForSequenceClassification.from_pretrained('path/path/to/save/model')\n",
    "loaded_tokenizer = DistilBertTokenizer.from_pretrained('path/path/to/save/tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b78d7b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Инициализация TF-IDF векторизатора\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Преобразование текста в TF-IDF представление\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "\n",
    "# Инициализация и обучение Logistic Regression\n",
    "lr_classifier = LogisticRegression()\n",
    "lr_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Предсказание и оценка\n",
    "lr_preds = lr_classifier.predict(X_val_tfidf)\n",
    "accuracy_lr = accuracy_score(y_val, lr_preds)\n",
    "\n",
    "print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad5fe2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задаем устройство (CPU или GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Определение простой RNN модели\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "82e4e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция обучения модели\n",
    "def train_rnn_model(model, train_loader, optimizer, criterion, num_epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in tqdm(train_loader, desc=f\"Training (Epoch {epoch + 1}/{num_epochs})\", leave=False):\n",
    "            inputs, labels = batch.text, batch.sentiment\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cfa27958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция оценки модели\n",
    "def evaluate_rnn_model(model, val_loader):\n",
    "    model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "            inputs, labels = batch.text, batch.sentiment\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            val_preds.extend(predictions)\n",
    "            val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2de31455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|█████████████████████████████████████████████████████████████████| 2400/2400 [00:04<00:00, 519.13it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-113-f27297e402ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Tokenize data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mtrain_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_attention_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mval_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_attention_masks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df, tokenizer, and other necessary code is already loaded\n",
    "\n",
    "# Map sentiment labels to numerical values\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize data\n",
    "max_len = 128\n",
    "train_input_ids, train_attention_masks, train_labels = tokenize_data(X_train, y_train, max_len)\n",
    "val_input_ids, val_attention_masks, val_labels = tokenize_data(X_val, y_val, max_len)\n",
    "\n",
    "# Create RNN model\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Parameters\n",
    "input_size = len(tokenizer.get_vocab())\n",
    "hidden_size = 128\n",
    "output_size = 2  # binary classification (positive or negative)\n",
    "\n",
    "# Create model, loss, and optimizer\n",
    "rnn_model = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion_rnn = nn.CrossEntropyLoss()\n",
    "optimizer_rnn = torch.optim.AdamW(rnn_model.parameters(), lr=2e-4)\n",
    "\n",
    "# Train RNN model\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rnn_model.train()\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\", leave=False):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        optimizer_rnn.zero_grad()\n",
    "\n",
    "        # Flatten input_ids to match the embedding layer\n",
    "        input_ids = input_ids.view(-1, max_len)\n",
    "\n",
    "        outputs = rnn_model(input_ids)\n",
    "        loss = criterion_rnn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_rnn.step()\n",
    "\n",
    "    rnn_model.eval()\n",
    "    val_preds = []\n",
    "    val_true = []\n",
    "\n",
    "    for batch in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "\n",
    "        # Flatten input_ids to match the embedding layer\n",
    "        input_ids = input_ids.view(-1, max_len)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = rnn_model(input_ids)\n",
    "\n",
    "        predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        val_preds.extend(predictions)\n",
    "        val_true.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true, val_preds)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} | Validation Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986caef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
